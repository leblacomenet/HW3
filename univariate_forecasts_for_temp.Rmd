---
title: "Univariate Forecasts for min_temp_val and heat_days_val"
output:
  pdf_document: default
  html_notebook: default
---

In this part we need to evaluate univariate forecasts of the temperatures for the multiple linear regression. 
We will thus need to use the predicted values : 
- of min_temp_val and heat_days_val from February 2017 to July 2017
- of extreme_heat and extreme_cold from March 2017 to July 2017. 
Here, we will only predict min_temp_val and heat_days_val. In another notebook, we will predict extreme_heat and extreme_cold. 

### Load the data
```{r}
library(readr)
temp <- na.omit(read_csv("data/temp.csv", col_types = cols(date = col_date(format = "%Y-%m-%d"))))
rmse <- function(l, r) {
  sqrt(sum((l - r)^2)/NROW(l))
}
exp_rmse <- function(l, r) {
  rmse(exp(l), exp(r))
}
```

# min_temp_val

### Stationarity and integration of the min_temp_val
```{r}
library(forecast)
tsdisplay(temp$min_temp_val,main="min_temp_val")
tsdisplay(diff(temp$min_temp_val, 3), main="Third Order Differientiation")
tsdisplay(diff(temp$min_temp_val, 12), main="Twelfth Order Differientiation")
```
As we can see, we need to do a 12th order differenciation in order to remove the seasonality. 

### Models evaluation
Let us convert data to a time series
```{r}
smin_temp_val = ts(temp$min_temp_val, start = c(1895, 1), frequency=12)
train = window(smin_temp_val, start = c(1895, 1), end = c(2016, 1))
test = window(smin_temp_val, start = c(2016,2), end = c(2017, 1))
```

## Simple Snaive
We first start with a simple seasonal naive process (ie. repetition of the last temporality). 
```{r}
snaive = snaive(train, h = 12)
accuracy(snaive, test)
```

## Time Series Decomposition with min_temp_val
Now for Time Series decomposition.
```{r}
decomp = stl(train, s.window="periodic")
plot(decomp)
```

As previously, our first attempt will just forecast the time series by removing seasonality, and then using the last observation to which we add back the seasonality as the next forecast value.

```{r}
stlfk_naive = forecast(decomp, method="naive", h=12)
summary(stlfk_naive)
accuracy(stlfk_naive, test)
```

Let's try with exponential smoothing to forecast the seasonally-adjusted series.

```{r}
stlfk_ets = forecast(decomp, method="ets", h=12)
summary(stlfk_ets)
accuracy(stlfk_ets, test)
```

As a last attempt, we can also use arima on the seasonally-adjusted data.

```{r}
stlfk_arima = forecast(decomp, method="arima", h=12)
summary(stlfk_arima)
accuracy(stlfk_arima, test)
```

R picked an ARIMA(2,1,2). 

The best model for now when we consider RMSE on the Test set is the STL +  Random walk. 

## Seasonal ARIMA

### Auto Arima on the log data
We first look at the auto.arima output. 
```{r}
autofit = auto.arima(train, seasonal=TRUE)
print(autofit)
autofk = forecast(autofit,h=12)
accuracy(autofk,test)
```

Let us now try with our own seasonal ARIMA. Based on the ACF PACF of the annual differentiation, we try with an  ARIMA(1,0,5)(0,0,2).

### Custom Seasonal Arima on the data
```{r}
custfit = Arima(train, order=c(1,0,5), seasonal=c(0,0,2))
print(custfit)
custfk = forecast(custfit,h=12)
accuracy(custfk,test)
```

Although the statistical tests and criterion seem better here, the error on train and test sets are higher, so we keep the STL +  Random walk.  

### Rolling Window Evaluation
To try more model, we would like to setup a rolling window. Let us find the optimal size of that window using the STL +  Random walk we just fitted.

```{r}
library(ggplot2)
err = data.frame("RMSE" = rep(0, 2014-1895+1), row.names = seq(1895, 2014))
for (y in 1895:2014){
    model = stl(window(train, start=c(y,1)), s.window="periodic")
    rmse = accuracy(forecast(model, method="naive", h=12),test)[2,"RMSE"]
    err[as.character(y),] = rmse
}
ggplot(err)+geom_line(aes(x=as.numeric(row.names(err)), y=RMSE))
```

Let us take a closer look at the years between 1950 and 1980. 
```{r}
ggplot(err)+geom_line(aes(x=as.numeric(row.names(err)), y=RMSE))+xlim(1950, 1980)
```

Let's take 1961, that is to say 55 years ! 

```{r}
errm = data.frame("RMSE" = rep(0, 12), row.names = seq(1, 12))
for (m in 1:12){
    model = stl(window(train, start=c(1961,m)), s.window="periodic")
    rmse = accuracy(forecast(model, method="naive", h=12),test)[2,"RMSE"]
    errm[as.character(m),] = rmse
}
ggplot(errm)+geom_line(aes(x=as.numeric(row.names(errm)), y=RMSE))+scale_x_continuous(breaks=seq(1,12))
```

We will thus use a rolling window of 7 full years.

```{r}
rollwitme = function(lgas){
    RMSE = rep(0,66)
    for (y in 1895:1961){
        tr = window(lgas, start=c(y,2), end=c(y+54,1))
        te = window(lgas, start=c(y+54,2), end=c(y+55,1))
        model = stl(tr, s.window="periodic")
        RMSE[y+1-1895] = accuracy(forecast(model, method="naive", h=12),te)[2,"RMSE"] 
    }
    return(RMSE)
}
```

```{r}
plot(1950:2016, rollwitme(smin_temp_val), type = 'l', main = "RMSE of a 55 year training set STL + Random walk forecast", ylab = "RMSE", xlab = "Forecasted year")
```

We notice a huge variability in the forecast performances over time ! 

Creation of the needed forecast : from Feb. 2017 to Feb. 2018 (we will only use from Feb. to July 2017). 
```{r}
train_final = window(smin_temp_val, start=c(1962,2), end=c(1962+55,1))
model = stl(train_final, s.window="periodic")
yearly_forecast = forecast(model, method="naive", h=12)
plot(yearly_forecast)
```

# heat_days_val

### Stationarity and integration of the heat_days_val
```{r}
library(forecast)
tsdisplay(temp$heat_days_val,main="heat_days_val")
tsdisplay(diff(temp$heat_days_val, 3), main="Third Order Differientiation")
tsdisplay(diff(temp$heat_days_val, 12), main="Twelfth Order Differientiation")
```

As previously, we need to do a 12th order differenciation in order to remove the seasonality. 

### Models evaluation
Let us convert data to a time series
```{r}
sheat_days_val = ts(temp$heat_days_val, start = c(1895, 1), frequency=12)
train = window(sheat_days_val, start = c(1895, 1), end = c(2016, 1))
test = window(sheat_days_val, start = c(2016,2), end = c(2017, 1))
```

## Simple Snaive
We first start with a simple seasonal naive process (ie. repetition of the last temporality). 
```{r}
snaive = snaive(train, h = 12)
accuracy(snaive, test)
```


## Time Series Decomposition with min_temp_val
Now for Time Series decomposition.
```{r}
decomp = stl(train, s.window="periodic")
plot(decomp)
```

As previously, our first attempt will just forecast the time series by removing seasonality, and then using the last observation to which we add back the seasonality as the next forecast value.

```{r}
stlfk_naive = forecast(decomp, method="naive", h=12)
summary(stlfk_naive)
accuracy(stlfk_naive, test)
```

Let's try with exponential smoothing to forecast the seasonally-adjusted series.

```{r}
stlfk_ets = forecast(decomp, method="ets", h=12)
summary(stlfk_ets)
accuracy(stlfk_ets, test)
```

As a last attempt, we can also use arima on the seasonally-adjusted data.

```{r}
stlfk_arima = forecast(decomp, method="arima", h=12)
summary(stlfk_arima)
accuracy(stlfk_arima, test)
```

R picked an ARIMA(0,1,3). 

The best model for now when we consider RMSE on the Test set is still the STL +  Random walk. 

## Seasonal ARIMA

### Auto Arima on the log data
We first look at the auto.arima output. 
```{r}
autofit = auto.arima(train, seasonal=TRUE)
print(autofit)
autofk = forecast(autofit,h=12)
accuracy(autofk,test)
```

Let us now try with our own seasonal ARIMA. Based on the ACF PACF of the annual differentiation, we try with an  ARIMA(1,0,5)(0,0,2).

### Custom Seasonal Arima on the data
```{r}
custfit = Arima(train, order=c(1,0,5), seasonal=c(0,0,2))
print(custfit)
custfk = forecast(custfit,h=12)
accuracy(custfk,test)
```

Here again, we keep the STL + Random walk.  

### Rolling Window Evaluation
To try more model, we would like to setup a rolling window. Let us find the optimal size of that window using the STL + Random walk we just fitted.

```{r}
library(ggplot2)
err = data.frame("RMSE" = rep(0, 2014-1895+1), row.names = seq(1895, 2014))
for (y in 1895:2014){
    model = stl(window(train, start=c(y,1)), s.window="periodic")
    rmse = accuracy(forecast(model, method="naive", h=12),test)[2,"RMSE"]
    err[as.character(y),] = rmse
}
ggplot(err)+geom_line(aes(x=as.numeric(row.names(err)), y=RMSE))
```

Let us take a closer look at the years between 1950 and 1980. 
```{r}
ggplot(err)+geom_line(aes(x=as.numeric(row.names(err)), y=RMSE))+xlim(1950, 1980)
```

Let us take a closer look at the year 1976. 

```{r}
errm = data.frame("RMSE" = rep(0, 12), row.names = seq(1, 12))
for (m in 1:12){
    model = stl(window(train, start=c(1976,m)), s.window="periodic")
    rmse = accuracy(forecast(model, method="naive", h=12),test)[2,"RMSE"]
    errm[as.character(m),] = rmse
}
ggplot(errm)+geom_line(aes(x=as.numeric(row.names(errm)), y=RMSE))+scale_x_continuous(breaks=seq(1,12))
```

We will still use a rolling window of 40 full years !

```{r}
rollwitme = function(lgas){
    RMSE = rep(0,81)
    for (y in 1895:1976){
        tr = window(lgas, start=c(y,2), end=c(y+39,1))
        te = window(lgas, start=c(y+39,2), end=c(y+40,1))
        model = stl(tr, s.window="periodic")
        RMSE[y+1-1895] = accuracy(forecast(model, method="naive", h=12),te)[2,"RMSE"] 
    }
    return(RMSE)
}
```


```{r}
plot(1935:2016, rollwitme(sheat_days_val), type = 'l', main = "RMSE of a 40 year training set STL + Random walk forecast", ylab = "RMSE", xlab = "Forecasted year")
```

Here again we notice a huge variability in the forecast performances ! 

Creation of the needed forecast : from Feb. 2017 to Feb. 2018 (we will only use from Feb. to July 2017). 
```{r}
train_final = window(sheat_days_val, start=c(1977,3), end=c(1977+40,2))
model = stl(train_final, s.window="periodic")
yearly_forecast = forecast(model, method="naive", h=12)
plot(yearly_forecast)
```
Warning : Since here there are negative predictions, we will replace all negative predictions with 0 !