---
title: "Univariate Forecasts for Temperatures"
output: html_notebook
---

In this part we evaluate univariate forecasts of the temperatures, needed for the multiple linear regression. 
We will thus need to use the predicted values : 
- of min_temp_val and heat_days_val from February 2017 to July 2017
- of extreme_heat and extreme_cold from March 2017 to July 2017. 

### Load the data
```{r}
library(readr)
temp <- na.omit(read_csv("data/temp.csv", col_types = cols(date = col_date(format = "%Y-%m-%d"))))
rmse <- function(l, r) {
  sqrt(sum((l - r)^2)/NROW(l))
}
exp_rmse <- function(l, r) {
  rmse(exp(l), exp(r))
}
```

## min_temp_val

### Stationarity and integration of the min_temp_val
```{r}
library(forecast)
tsdisplay(temp$min_temp_val,main="min_temp_val")
tsdisplay(diff(temp$min_temp_val, 3), main="Third Order Differientiation")
tsdisplay(diff(temp$min_temp_val, 12), main="Twelfth Order Differientiation")
```
COMMENTS TO DO. 

### Models evaluation
Let us convert data to a time series
```{r}
smin_temp_val = ts(temp$min_temp_val, start = c(1973, 1), frequency=12)
train = window(smin_temp_val, start = c(1973, 1), end = c(2015, 6))
test = window(smin_temp_val, start = c(2015,7), end = c(2016, 6))
```

## Time Series Decomposition with min_temp_val

```{r}
decomp = stl(train, s.window="periodic")
plot(decomp)
```

Our first attempt will just forecast the time series by removing seasonality, and then using the last observation to which we add back the seasonality as the next forecast value.

```{r}
stlfk_naive = forecast(decomp, method="naive", h=12)
summary(stlfk_naive)
accuracy(stlfk_naive, test)
```

Let's try with exponential smoothing to forecast the seasonally-adjusted series.

```{r}
stlfk_ets = forecast(decomp, method="ets", h=12)
summary(stlfk_ets)
accuracy(stlfk_ets, test)
```

As a last attempt, we can also use arima on the seasonally-adjusted data.

```{r}
stlfk_arima = forecast(decomp, method="arima", h=12)
summary(stlfk_arima)
accuracy(stlfk_arima, test)
```

R picked an ARIMA(1,0,1). The results aren't as good as with the naive approach either. To conclude on this approach, it seems that the best model is to do a naive forecast on the seasonally-adjusted data.

We should note that what we are interested in is the RMSE on the test set for the values of the gas consumption, and not for their log.
```{r}
c(exp_rmse(stlfk_naive$mean, test), exp_rmse(stlfk_ets$mean, test), exp_rmse(stlfk_arima$mean, test))
```

According to this criterion, the model to pick is the ARIMA one.

## Time Series Decomposition with direct gas consumption

The approach is the same as above, but this time we fit the models to the data directly and not to the log of it. It is equivalent to doing an additive decomposition.

```{r}
ddecomp = stl(dtrain, s.window="periodic")
plot(ddecomp)
```

Just as in the previous case, graphically the decomposition seems interesting. The trend seems a bit harder to spot in this one. There seems to be extreme peaks which may be problematic for our forecasts.

Let's fit the naive, exponential smoothing and arima models now.

```{r}
dstl_naive = forecast(ddecomp, method="naive", h=12)
dstl_ets = forecast(ddecomp, method="ets", h=12)
dstl_arima = forecast(ddecomp, method="arima", h=12)

summary(dstl_naive)
summary(dstl_ets)
summary(dstl_arima)

accuracy(dstl_naive, dtest)
accuracy(dstl_arima, dtest)
accuracy(dstl_ets, dtest)
```

This time an ARIMA(1, 0, 0) is picked. The best performing model is the exponential smoothing one (ets). This additive approach performs better than the multiplicative approach we outlined in the previous section.

## Seasonal ARIMA

### Auto Arima on the log data
```{r}
autofit = auto.arima(train, seasonal=TRUE)
print(autofit)
autofk = forecast(autofit,h=12)
accuracy(autofk,test)
```

Let us now try with our own seasonal ARIMA. Based on the ACF PACF of the annual differentiation, we try with an  ARIMA(4,0,1)(0,1,2).

### Custom Seasonal Arima on the log data
```{r}
custfit = Arima(train, order=c(4,0,1), seasonal=c(0,1,2))
print(custfit)
custfk = forecast(custfit,h=12)
accuracy(custfk,test)
```

Although the statistical tests and criterion seem better here, the error on train and test sets are higher, so we keep the auto ARIMA.

### Seasonal Arima on the actual data
```{r}
dautofit = auto.arima(dtrain, seasonal=TRUE)
print(dautofit)
dautofk = forecast(dautofit,h=12)
accuracy(dautofk,dtest)
```

This time the RMSE is quite high. Comparing with the results on the log data gives the following RMSE:
```{r}
c(accuracy(dautofk,dtest)[2, "RMSE"], exp_rmse(autofk$mean, test))
```
The seasonal ARIMA is not doing any good on the actual data, compared to the log data.

### Rolling Window Evaluation
To try more model, we would like to setup a rolling window. Let us find the optimal size of that window using the Auto ARIMA we just fitted.
```{r}
library(ggplot2)
err = data.frame("RMSE" = rep(0, 2010-1973+1), row.names = seq(1973, 2010))
for (y in 1973:2010){
    model = Arima(window(train, start=c(y,1)), order=c(2,0,0), seasonal=c(1,1,1), include.drift = TRUE)
    rmse = accuracy(forecast(model, h=12),test)[2,"RMSE"]
    err[as.character(y),] = rmse
}
ggplot(err)+geom_line(aes(x=as.numeric(row.names(err)), y=RMSE))
```

Let us take a closer look at the year 2000.

```{r}
errm = data.frame("RMSE" = rep(0, 12), row.names = seq(1, 12))
for (m in 1:12){
    model = Arima(window(train, start=c(2000,m)), order=c(2,0,0), seasonal=c(1,1,1), include.drift = TRUE)
    rmse = accuracy(forecast(model, h=12),test)[2,"RMSE"]
    errm[as.character(m),] = rmse
}
ggplot(errm)+geom_line(aes(x=as.numeric(row.names(errm)), y=RMSE))+scale_x_continuous(breaks=seq(1,12))
```

We will thus use a rolling window of 15 full years.

```{r}
rollwitme = function(p,d,q,P,D,Q, drift){
    RMSE = rep(0,28)
    for (y in 1973:2000){
        tr = window(lgas, start=c(y,7), end=c(y+15,6))
        te = window(lgas, start=c(y+15,7), end=c(y+16,6))
        model = Arima(tr, order=c(p,d,q), seasonal=c(P,D,Q), include.drift = drift)
        RMSE[y+1-1973] = accuracy(forecast(model, h=12),te)[2,"RMSE"] 
    }
    return(RMSE)
}
```


```{r}
eval = data.frame('run'=seq(1:28))
eval['arima_v1'] = rollwitme(2,0,0,1,1,1,TRUE)
eval['arima_v2'] = rollwitme(1,0,0,0,1,1,FALSE)
eval['arima_v3'] = rollwitme(1,0,0,0,1,1,TRUE)
eval['arima_v4'] = rollwitme(2,0,0,1,1,1,FALSE)
eval['arima_v5'] = rollwitme(4,0,0,0,1,1,FALSE)
ggplot(stack(eval[,names(eval) != "run"]))+geom_boxplot(aes(x=ind, y=values))+
    labs(x='model', title="Model Performances", y="RMSE")
```

