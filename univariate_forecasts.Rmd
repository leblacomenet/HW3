---
title: "Univariate Forecasts"
output: html_notebook
---

In this part we evaluate univariate forecasts of the gas consumption.

## Graphical Analysis and stationarity

---

### Load useful packages & data
```{r}
library(readr)
library(forecast)
library(ggplot2)
library(knitr)
gas <- na.omit(read_csv("data/gas.csv", col_types = cols(date = col_date(format = "%Y-%m-%d"))))
```
We also define functions for the RMS error which we will use as the main evaluation metric here.
```{r}
rmse <- function(l, r) {sqrt(sum((l - r)^2)/NROW(l))}
exp_rmse <- function(l, r) {rmse(exp(l), exp(r))}
```


### Stationarity and integration of the consumption
Let us plot the values, ACF and PACF of the data and its logarithm, with seasonal and annual differentiation.
```{r}
tsdisplay(gas$gas_cons,main="Gas Consumption")
tsdisplay(diff(gas$gas_cons, 3), main="Third Order Differientiation")
tsdisplay(diff(gas$gas_cons, 12), main="Twelfth Order Differientiation")
```
The results are similar to that of the log of the consumption. Annual differencing seem to yield a somewhat stationary process with no obvious seasonality.

### Stationarity and integration of the log of the consumption
```{r}
tsdisplay(gas$log_gas_cons,main="Log transform of Gas Consumption")
tsdisplay(diff(gas$log_gas_cons, 3), main="Third Order Differientiation")
tsdisplay(diff(gas$log_gas_cons, 12), main="Twelfth Order Differientiation")
```
Unsurprisingly, the results are globally similar: we notice a strong seasonality, still observable after seasonal differencing. Exploring further the dynamics of the log consumption with annual differencing yields a time series that looks stationary.

Let us perform statistical tests on the log gas consumption for stationarity with differencing and seasonal differencing:

```{r}
ndiffs(gas$log_gas_cons, alpha=0.05, test=c("kpss","adf", "pp"), max.d=2)
nsdiffs(gas$log_gas_cons, m=12, test=c("ocsb","ch"), max.D=1)
```
The unit root test with level 0.05 indicates there is no need for differencing to obtain a stationary time series, but here the strong seasonality makes it irrelevant. We thus look at seasonal differencing which shows there is a need for first order seasonal differencing for the series to become stationary.

### Pre-processing
Let us convert data into time series and split it into a training set and a one year test set to evaluate our first models.
```{r}
lgas = ts(gas$log_gas_cons, start = c(1973, 1), frequency=12)
train = window(lgas, start = c(1973, 1), end = c(2015, 6))
test = window(lgas, start = c(2015,7), end = c(2016, 6))

dgas = ts(gas$gas_cons, start = c(1973, 1), frequency=12)
dtrain = window(dgas, start = c(1973, 1), end = c(2015, 6))
dtest = window(dgas, start = c(2015, 7), end = c(2016, 6))
```

## Time Series Decomposition with log gas consumption

---

Let's try to decompose our series in three components: trend, seasonality and error. We assume seasonality is fixed across years and we do our analysis on the log of consumption data to avoid overweighing extreme values. This is a strong assumption that is equivalent to doing a multiplicative decomposition.

```{r}
decomp = stl(train, s.window="periodic")
plot(decomp)
```

The stl decomposition seems to capture well the seasonality "graphically". We'll see how well this translates into forecasts. Our first attempt will just forecast the time series by removing seasonality, and then using the last observation to which we add back the seasonality as the next forecast value.

```{r}
stlfk_naive = forecast(decomp, method="naive", h=12)
summary(stlfk_naive)
accuracy(stlfk_naive, test)
```

Let's try with exponential smoothing to forecast the seasonally-adjusted series.

```{r}
stlfk_ets = forecast(decomp, method="ets", h=12)
summary(stlfk_ets)
accuracy(stlfk_ets, test)
```

It actually does worse than the naive technique. As a last attempt, we can also use arima on the seasonally-adjusted data.

```{r}
stlfk_arima = forecast(decomp, method="arima", h=12)
summary(stlfk_arima)
accuracy(stlfk_arima, test)
```

R picked an ARIMA(3,1,2). The results aren't as good as with the naive approach either. To conclude on this approach, it seems that the best model is to do a naive forecast on the seasonally-adjusted data.

We should note that what we are interested in is the RMSE on the test set for the values of the gas consumption, and not for their log.
```{r}
kable(data.frame("naive"=exp_rmse(stlfk_naive$mean, test), 
           "ets"=exp_rmse(stlfk_ets$mean, test), 
           "arima"=exp_rmse(stlfk_arima$mean, test), row.names = "RMSE"), align='c')
```

According to this criterion, the model to pick is the ARIMA one.

## Time Series Decomposition with direct gas consumption

---

The approach is the same as above, but this time we fit the models to the data directly and not to the log of it. It is equivalent to doing an additive decomposition.

```{r}
ddecomp = stl(dtrain, s.window="periodic")
plot(ddecomp)
```

Just as in the previous case, graphically the decomposition seems interesting. The trend seems a bit harder to spot in this one. There seems to be extreme peaks which may be problematic for our forecasts.

Let's fit the naive, exponential smoothing and arima models now.

```{r}
dstl_naive = forecast(ddecomp, method="naive", h=12)
dstl_ets = forecast(ddecomp, method="ets", h=12)
dstl_arima = forecast(ddecomp, method="arima", h=12)

summary(dstl_naive)
summary(dstl_ets)
summary(dstl_arima)

accuracy(dstl_naive, dtest)
accuracy(dstl_arima, dtest)
accuracy(dstl_ets, dtest)
```

This time an ARIMA(1, 0, 0) is picked. The best performing model is the exponential smoothing one (ets). This additive approach performs better than the multiplicative approach we outlined in the previous section.

## Seasonal ARIMA

---

Let us now try a different approach and forecast the log gas consumption with Seasonal ARIMA Models.

### Auto Arima on the log data
```{r}
autofit = auto.arima(train, seasonal=TRUE)
print(autofit)
tsdisplay(residuals(autofit))
autofk = forecast(autofit,h=12)
accuracy(autofk,test)
```
The ACF and PACF of the residuals show no significant spikes.


Let us now try with our own seasonal ARIMA. Based on the ACF PACF of the annual differentiation, we try with an  ARIMA(4,0,1)(0,1,2).

### Custom Seasonal Arima on the log data
```{r}
custfit = Arima(train, order=c(4,0,1), seasonal=c(0,1,1))
print(custfit)
tsdisplay(residuals(custfit))
custfk = forecast(custfit,h=12)
accuracy(custfk,test)
```
Again, we find no alarming spikes in the residuals. But this model is outperformed by the the one found automatically.

### Seasonal Arima on the actual data
```{r}
dautofit = auto.arima(dtrain, seasonal=TRUE)
print(dautofit)
dautofk = forecast(dautofit,h=12)
accuracy(dautofk,dtest)
```

This time the RMSE is quite high. Comparing with the results on the log data gives the following RMSE:
```{r}
kable(data.frame("Direct"=accuracy(dautofk,dtest)[2, "RMSE"], 
           "Logarithm"=exp_rmse(autofk$mean, test), row.names="RMSE"), align='c')
```
The seasonal ARIMA is not doing any good on the actual data, compared to the log data.

### Rolling Window Evaluation
To compare models more precisely, we would like to setup a rolling window evaluation. Let us find the optimal size of that window using the Auto ARIMA we just fitted, by comparing the performances on RMSE of models with different training test time span.
```{r}
err = data.frame("RMSE" = rep(0, 2010-1973+1), row.names = seq(1973, 2010))
for (y in 1973:2010){
    model = Arima(window(train, start=c(y,1)), order=c(2,0,0), seasonal=c(1,1,1), include.drift = TRUE)
    rmse = accuracy(forecast(model, h=12),test)[2,"RMSE"]
    err[as.character(y),] = rmse
}
ggplot(err)+geom_line(aes(x=as.numeric(row.names(err)), y=RMSE))
```

Let us take a closer look at the year 2000.

```{r}
errm = data.frame("RMSE" = rep(0, 12), row.names = seq(1, 12))
for (m in 1:12){
    model = Arima(window(train, start=c(2000,m)), order=c(2,0,0), seasonal=c(1,1,1), include.drift = TRUE)
    rmse = accuracy(forecast(model, h=12),test)[2,"RMSE"]
    errm[as.character(m),] = rmse
}
ggplot(errm)+geom_line(aes(x=as.numeric(row.names(errm)), y=RMSE))+scale_x_continuous(breaks=seq(1,12))
```

We will thus use a rolling window of 15 years, and define the R function below:

```{r}
roll = function(p,d,q,P,D,Q, drift){
    RMSE = rep(0,28)
    for (y in 1973:2000){
        tr = window(lgas, start=c(y,7), end=c(y+15,6))
        te = window(lgas, start=c(y+15,7), end=c(y+16,6))
        model = Arima(tr, order=c(p,d,q), seasonal=c(P,D,Q), include.drift = drift)
        RMSE[y+1-1973] = accuracy(forecast(model, h=12),te)[2,"RMSE"] 
    }
    return(RMSE)
}
```

We can now, evaluate models predictions on different years. We try below different seasonal arima, reducing the number of lags little by little.
```{r}
eval = data.frame('run'=seq(1:28))
eval['arima_v1'] = roll(2,0,0,1,1,1,TRUE)
eval['arima_v2'] = roll(4,0,0,0,1,1,FALSE)
eval['arima_v3'] = roll(2,0,0,1,1,1,FALSE)
eval['arima_v4'] = roll(1,0,0,0,1,1,TRUE)
eval['arima_v5'] = roll(1,0,0,0,1,1,FALSE)
ggplot(stack(eval[,names(eval) != "run"]))+geom_boxplot(aes(x=ind, y=values))+
    labs(x='model', title="Model Performances", y="RMSE")
```

All model have outliers. We keep the model v2, i.e. SARIMA((4,0,0),(0,1,1),12). We compute the RMSE on the exponential of the predictions of the last year to compare with direct predictions obtained earlier on:

```{r}
finalModel = Arima(train, order=c(4,0,0), seasonal=c(0,1,1), include.drift = FALSE)
finalFkst = forecast(finalModel, 12)
accuracy(finalFkst, test)[2,"RMSE"]
exp_rmse(finalFkst$mean, test)
```

The seasonal ARIMA on the log consumption is outperformed by the times series decomposition approach.
